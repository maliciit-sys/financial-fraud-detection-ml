{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ce2b166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Using device: cuda\n",
      "============================================================\n",
      "MODEL CONFIGURATION\n",
      "============================================================\n",
      "Input size: 22\n",
      "Hidden sizes: [256, 128, 64]\n",
      "Dropout rate: 0.3\n",
      "Best F1 (training): 0.2134\n",
      "Epochs trained: 11\n",
      "\n",
      "============================================================\n",
      "ENCODERS LOADED\n",
      "============================================================\n",
      "Gender mapping: {'M': 1, 'F': 0}\n",
      "Category columns: 14 categories\n",
      "State encoding: 51 states\n",
      "\n",
      "✓ Scaler loaded\n",
      "\n",
      "✓ Model loaded and set to evaluation mode\n",
      "Total parameters: 48,001\n",
      "Loading test dataset...\n",
      "Test set shape: (555719, 23)\n",
      "Test set fraud rate: 0.39%\n",
      "\n",
      "Preprocessing test data...\n",
      "Preprocessed features shape: (555719, 22)\n",
      "Feature columns: ['amt', 'gender', 'city_pop', 'hour', 'day_of_week', 'month', 'age', 'state_encoded', 'cat_entertainment', 'cat_food_dining', 'cat_gas_transport', 'cat_grocery_net', 'cat_grocery_pos', 'cat_health_fitness', 'cat_home', 'cat_kids_pets', 'cat_misc_net', 'cat_misc_pos', 'cat_personal_care', 'cat_shopping_net', 'cat_shopping_pos', 'cat_travel']\n",
      "Batch size: 2048\n",
      "Number of batches: 272\n",
      "\n",
      "============================================================\n",
      "RUNNING INFERENCE\n",
      "============================================================\n",
      "Processed batch 50/272\n",
      "Processed batch 100/272\n",
      "Processed batch 150/272\n",
      "Processed batch 200/272\n",
      "Processed batch 250/272\n",
      "\n",
      "✓ Inference complete!\n",
      "Total predictions: 555,719\n",
      "\n",
      "============================================================\n",
      "PREDICTION STATISTICS\n",
      "============================================================\n",
      "Probability range: 0.000000 to 0.998469\n",
      "Probability mean: 0.071401\n",
      "Probability median: 0.003358\n",
      "Probability std: 0.162215\n",
      "\n",
      "Probability distribution:\n",
      "  < 0.1:  458,381 (82.48%)\n",
      "  0.1-0.3: 45,818 (8.24%)\n",
      "  0.3-0.5: 26,700 (4.80%)\n",
      "  0.5-0.7: 16,393 (2.95%)\n",
      "  0.7-0.9: 6,548 (1.18%)\n",
      "  >= 0.9: 1,879 (0.34%)\n",
      "\n",
      "============================================================\n",
      "PREDICTIONS AT DEFAULT THRESHOLD (0.5)\n",
      "============================================================\n",
      "Predicted Fraud: 24,820\n",
      "Predicted Non-Fraud: 530,899\n",
      "\n",
      "Actual Fraud: 2,145\n",
      "Actual Non-Fraud: 553,574\n",
      "\n",
      "Quick Metrics (threshold=0.5):\n",
      "  Accuracy:  0.9591\n",
      "  Precision: 0.0851\n",
      "  Recall:    0.9846\n",
      "  F1 Score:  0.1566\n",
      "\n",
      "✓ Saved: test_predictions.csv\n",
      "✓ Saved: test_probabilities.npy\n",
      "✓ Saved: test_labels.npy\n",
      "\n",
      "======================================================================\n",
      "INFERENCE SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Dataset: Non-PCA Primary (Sparkov)\n",
      "Model: Neural Network (22 features → 256 → 128 → 64 → 1)\n",
      "\n",
      "Test Set:\n",
      "- Total samples: 555,719\n",
      "- Actual fraud rate: 0.39%\n",
      "\n",
      "Predictions (threshold=0.5):\n",
      "- Predicted frauds: 24,820\n",
      "- Predicted non-frauds: 530,899\n",
      "\n",
      "Files saved:\n",
      "- outputs/non_pca_primary/neural_network/test_predictions.csv\n",
      "- outputs/non_pca_primary/neural_network/test_probabilities.npy\n",
      "- outputs/non_pca_primary/neural_network/test_labels.npy\n",
      "\n",
      "Next step: Run 04_non_pca_primary_evaluation.ipynb for:\n",
      "- Threshold optimization\n",
      "- Detailed metrics at various thresholds\n",
      "- Confusion matrix & ROC curves\n",
      "- Final model performance analysis\n",
      "\n",
      "\n",
      "============================================================\n",
      "SAMPLE PREDICTIONS (First 20)\n",
      "============================================================\n",
      " probability  prediction_default  actual\n",
      "      0.0288                   0     0.0\n",
      "      0.4031                   0     0.0\n",
      "      0.1253                   0     0.0\n",
      "      0.0162                   0     0.0\n",
      "      0.1343                   0     0.0\n",
      "      0.4956                   0     0.0\n",
      "      0.0051                   0     0.0\n",
      "      0.3319                   0     0.0\n",
      "      0.0021                   0     0.0\n",
      "      0.0569                   0     0.0\n",
      "      0.0007                   0     0.0\n",
      "      0.1143                   0     0.0\n",
      "      0.0003                   0     0.0\n",
      "      0.0002                   0     0.0\n",
      "      0.3299                   0     0.0\n",
      "      0.0003                   0     0.0\n",
      "      0.0025                   0     0.0\n",
      "      0.4300                   0     0.0\n",
      "      0.0003                   0     0.0\n",
      "      0.2507                   0     0.0\n",
      "\n",
      "============================================================\n",
      "HIGH-CONFIDENCE FRAUD PREDICTIONS (prob >= 0.9)\n",
      "============================================================\n",
      "Count: 1,879\n",
      "Actually fraud: 1,649 (87.8%)\n",
      "\n",
      "============================================================\n",
      "UNCERTAIN PREDICTIONS (0.4 <= prob <= 0.6)\n",
      "============================================================\n",
      "Count: 20,951 (3.77% of total)\n",
      "Actually fraud: 40 (0.2%)\n",
      "\n",
      "======================================================================\n",
      "INFERENCE COMPLETE!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# NOTEBOOK 03: MODEL INFERENCE - NON-PCA PRIMARY (SPARKOV)\n",
    "# =============================================================================\n",
    "# Author: Muhammad Ali Tahir\n",
    "# MS Data Science Program, Superior University Lahore\n",
    "# Dataset: Sparkov Credit Card Transactions (Interpretable Features)\n",
    "# =============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# # 3. Model Inference - Sparkov Fraud Detection\n",
    "#\n",
    "# This notebook:\n",
    "# - Loads the trained Neural Network model\n",
    "# - Makes predictions on the test dataset\n",
    "# - Saves raw probabilities for evaluation\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.1 Import Libraries\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.2 Load Model Architecture\n",
    "\n",
    "# %%\n",
    "class FraudDetectionNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes=[256, 128, 64], dropout_rate=0.3):\n",
    "        super(FraudDetectionNN, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.3 Load Saved Artifacts\n",
    "\n",
    "# %%\n",
    "# Load model configuration\n",
    "with open('../../models/non_pca_primary/model_config.json', 'r') as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input size: {model_config['input_size']}\")\n",
    "print(f\"Hidden sizes: {model_config['hidden_sizes']}\")\n",
    "print(f\"Dropout rate: {model_config['dropout_rate']}\")\n",
    "print(f\"Best F1 (training): {model_config['best_f1']:.4f}\")\n",
    "print(f\"Epochs trained: {model_config['epochs_trained']}\")\n",
    "\n",
    "# %%\n",
    "# Load encoders\n",
    "with open('../../models/non_pca_primary/encoders.pkl', 'rb') as f:\n",
    "    encoders = pickle.load(f)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ENCODERS LOADED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Gender mapping: {encoders['gender']}\")\n",
    "print(f\"Category columns: {len(encoders['category_cols'])} categories\")\n",
    "print(f\"State encoding: {len(encoders['state'])} states\")\n",
    "\n",
    "# %%\n",
    "# Load scaler\n",
    "with open('../../models/non_pca_primary/scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "print(\"\\n✓ Scaler loaded\")\n",
    "\n",
    "# %%\n",
    "# Load model\n",
    "model = FraudDetectionNN(\n",
    "    input_size=model_config['input_size'],\n",
    "    hidden_sizes=model_config['hidden_sizes'],\n",
    "    dropout_rate=model_config['dropout_rate']\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load('../../models/non_pca_primary/nn_model.pth', map_location=DEVICE))\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(\"\\n✓ Model loaded and set to evaluation mode\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.4 Define Preprocessing Function\n",
    "\n",
    "# %%\n",
    "def preprocess_for_inference(df, encoders, scaler):\n",
    "    \"\"\"\n",
    "    Preprocess new data for inference using saved encoders and scaler.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # ========== 1. TEMPORAL FEATURES ==========\n",
    "    df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "    df['hour'] = df['trans_date_trans_time'].dt.hour\n",
    "    df['day_of_week'] = df['trans_date_trans_time'].dt.dayofweek\n",
    "    df['month'] = df['trans_date_trans_time'].dt.month\n",
    "\n",
    "    # ========== 2. AGE CALCULATION ==========\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['age'] = (df['trans_date_trans_time'] - df['dob']).dt.days // 365\n",
    "\n",
    "    # ========== 3. DROP UNNECESSARY COLUMNS ==========\n",
    "    drop_cols = ['Unnamed: 0', 'trans_date_trans_time', 'cc_num', 'merchant',\n",
    "                 'first', 'last', 'street', 'city', 'zip', 'lat', 'long',\n",
    "                 'job', 'dob', 'trans_num', 'unix_time', 'merch_lat', 'merch_long']\n",
    "    df = df.drop(columns=[col for col in drop_cols if col in df.columns])\n",
    "\n",
    "    # ========== 4. ENCODE CATEGORICAL FEATURES ==========\n",
    "    # Gender\n",
    "    df['gender'] = df['gender'].map(encoders['gender']).astype(int)\n",
    "\n",
    "    # State: Target Encoding\n",
    "    df['state_encoded'] = df['state'].map(encoders['state']).fillna(encoders['state_default']).astype(float)\n",
    "    df = df.drop(columns=['state'])\n",
    "\n",
    "    # Category: One-Hot Encoding\n",
    "    category_dummies = pd.get_dummies(df['category'], prefix='cat')\n",
    "    for col in encoders['category_cols']:\n",
    "        if col not in category_dummies.columns:\n",
    "            category_dummies[col] = 0\n",
    "    category_dummies = category_dummies[encoders['category_cols']]\n",
    "\n",
    "    df = pd.concat([df.reset_index(drop=True), category_dummies.reset_index(drop=True)], axis=1)\n",
    "    df = df.drop(columns=['category'])\n",
    "\n",
    "    # ========== 5. EXTRACT TARGET IF EXISTS ==========\n",
    "    y = None\n",
    "    if 'is_fraud' in df.columns:\n",
    "        y = df['is_fraud'].values.astype(np.float32)\n",
    "        df = df.drop(columns=['is_fraud'])\n",
    "\n",
    "    # ========== 6. ENSURE NUMERIC & CORRECT ORDER ==========\n",
    "    X = df.astype(np.float32)\n",
    "\n",
    "    # ========== 7. SCALE NUMERIC FEATURES ==========\n",
    "    numeric_cols = ['amt', 'city_pop', 'age', 'hour', 'day_of_week', 'month', 'state_encoded']\n",
    "    numeric_cols = [col for col in numeric_cols if col in X.columns]\n",
    "    X[numeric_cols] = scaler.transform(X[numeric_cols])\n",
    "\n",
    "    # Ensure correct column order\n",
    "    X = X[model_config['feature_columns']]\n",
    "    X = X.astype(np.float32)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.5 Load Test Data\n",
    "\n",
    "# %%\n",
    "print(\"Loading test dataset...\")\n",
    "test_df = pd.read_csv('../../data/non_pca_primary/fraudTest.csv')\n",
    "\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "print(f\"Test set fraud rate: {test_df['is_fraud'].mean()*100:.2f}%\")\n",
    "\n",
    "# %%\n",
    "# Preprocess test data\n",
    "print(\"\\nPreprocessing test data...\")\n",
    "X_test, y_test = preprocess_for_inference(test_df, encoders, scaler)\n",
    "\n",
    "print(f\"Preprocessed features shape: {X_test.shape}\")\n",
    "print(f\"Feature columns: {X_test.columns.tolist()}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.6 Create DataLoader\n",
    "\n",
    "# %%\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        self.X = torch.FloatTensor(X.astype(np.float32))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx]\n",
    "\n",
    "# %%\n",
    "BATCH_SIZE = 2048  # Larger batch for faster inference\n",
    "test_dataset = InferenceDataset(X_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Number of batches: {len(test_loader)}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.7 Run Inference\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RUNNING INFERENCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_probabilities = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, X_batch in enumerate(test_loader):\n",
    "        X_batch = X_batch.to(DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "\n",
    "        all_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "        # Progress\n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            print(f\"Processed batch {batch_idx + 1}/{len(test_loader)}\")\n",
    "\n",
    "all_probabilities = np.array(all_probabilities)\n",
    "\n",
    "print(f\"\\n✓ Inference complete!\")\n",
    "print(f\"Total predictions: {len(all_probabilities):,}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.8 Prediction Statistics\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PREDICTION STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"Probability range: {all_probabilities.min():.6f} to {all_probabilities.max():.6f}\")\n",
    "print(f\"Probability mean: {all_probabilities.mean():.6f}\")\n",
    "print(f\"Probability median: {np.median(all_probabilities):.6f}\")\n",
    "print(f\"Probability std: {all_probabilities.std():.6f}\")\n",
    "\n",
    "# Distribution of probabilities\n",
    "print(f\"\\nProbability distribution:\")\n",
    "print(f\"  < 0.1:  {(all_probabilities < 0.1).sum():,} ({(all_probabilities < 0.1).mean()*100:.2f}%)\")\n",
    "print(f\"  0.1-0.3: {((all_probabilities >= 0.1) & (all_probabilities < 0.3)).sum():,} ({((all_probabilities >= 0.1) & (all_probabilities < 0.3)).mean()*100:.2f}%)\")\n",
    "print(f\"  0.3-0.5: {((all_probabilities >= 0.3) & (all_probabilities < 0.5)).sum():,} ({((all_probabilities >= 0.3) & (all_probabilities < 0.5)).mean()*100:.2f}%)\")\n",
    "print(f\"  0.5-0.7: {((all_probabilities >= 0.5) & (all_probabilities < 0.7)).sum():,} ({((all_probabilities >= 0.5) & (all_probabilities < 0.7)).mean()*100:.2f}%)\")\n",
    "print(f\"  0.7-0.9: {((all_probabilities >= 0.7) & (all_probabilities < 0.9)).sum():,} ({((all_probabilities >= 0.7) & (all_probabilities < 0.9)).mean()*100:.2f}%)\")\n",
    "print(f\"  >= 0.9: {(all_probabilities >= 0.9).sum():,} ({(all_probabilities >= 0.9).mean()*100:.2f}%)\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.9 Quick Prediction Preview (Default Threshold 0.5)\n",
    "\n",
    "# %%\n",
    "DEFAULT_THRESHOLD = 0.5\n",
    "y_pred_default = (all_probabilities >= DEFAULT_THRESHOLD).astype(int)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"PREDICTIONS AT DEFAULT THRESHOLD ({DEFAULT_THRESHOLD})\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"Predicted Fraud: {y_pred_default.sum():,}\")\n",
    "print(f\"Predicted Non-Fraud: {(y_pred_default == 0).sum():,}\")\n",
    "\n",
    "if y_test is not None:\n",
    "    print(f\"\\nActual Fraud: {y_test.sum():,.0f}\")\n",
    "    print(f\"Actual Non-Fraud: {(y_test == 0).sum():,.0f}\")\n",
    "\n",
    "    # Quick metrics\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred_default)\n",
    "    precision = precision_score(y_test, y_pred_default, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred_default, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred_default, zero_division=0)\n",
    "\n",
    "    print(f\"\\nQuick Metrics (threshold={DEFAULT_THRESHOLD}):\")\n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1 Score:  {f1:.4f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.10 Save Predictions\n",
    "\n",
    "# %%\n",
    "# Create predictions dataframe\n",
    "predictions_df = pd.DataFrame({\n",
    "    'probability': all_probabilities,\n",
    "    'prediction_default': y_pred_default\n",
    "})\n",
    "\n",
    "if y_test is not None:\n",
    "    predictions_df['actual'] = y_test\n",
    "\n",
    "# Save predictions\n",
    "predictions_df.to_csv('../../outputs/non_pca_primary/neural_network/test_predictions.csv', index=False)\n",
    "print(\"\\n✓ Saved: test_predictions.csv\")\n",
    "\n",
    "# %%\n",
    "# Save raw probabilities as numpy array (for faster loading in evaluation)\n",
    "np.save('../../outputs/non_pca_primary/neural_network/test_probabilities.npy', all_probabilities)\n",
    "print(\"✓ Saved: test_probabilities.npy\")\n",
    "\n",
    "if y_test is not None:\n",
    "    np.save('../../outputs/non_pca_primary/neural_network/test_labels.npy', y_test)\n",
    "    print(\"✓ Saved: test_labels.npy\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.11 Inference Summary\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INFERENCE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\"\"\n",
    "Dataset: Non-PCA Primary (Sparkov)\n",
    "Model: Neural Network (22 features → 256 → 128 → 64 → 1)\n",
    "\n",
    "Test Set:\n",
    "- Total samples: {len(all_probabilities):,}\n",
    "- Actual fraud rate: {y_test.mean()*100:.2f}%\n",
    "\n",
    "Predictions (threshold=0.5):\n",
    "- Predicted frauds: {y_pred_default.sum():,}\n",
    "- Predicted non-frauds: {(y_pred_default == 0).sum():,}\n",
    "\n",
    "Files saved:\n",
    "- outputs/non_pca_primary/neural_network/test_predictions.csv\n",
    "- outputs/non_pca_primary/neural_network/test_probabilities.npy\n",
    "- outputs/non_pca_primary/neural_network/test_labels.npy\n",
    "\n",
    "Next step: Run 04_non_pca_primary_evaluation.ipynb for:\n",
    "- Threshold optimization\n",
    "- Detailed metrics at various thresholds\n",
    "- Confusion matrix & ROC curves\n",
    "- Final model performance analysis\n",
    "\"\"\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.12 Sample Predictions\n",
    "\n",
    "# %%\n",
    "# Show sample predictions\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAMPLE PREDICTIONS (First 20)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sample_df = predictions_df.head(20).copy()\n",
    "sample_df['probability'] = sample_df['probability'].round(4)\n",
    "print(sample_df.to_string(index=False))\n",
    "\n",
    "# %%\n",
    "# Show high-confidence fraud predictions\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"HIGH-CONFIDENCE FRAUD PREDICTIONS (prob >= 0.9)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "high_conf_fraud = predictions_df[predictions_df['probability'] >= 0.9]\n",
    "print(f\"Count: {len(high_conf_fraud):,}\")\n",
    "\n",
    "if y_test is not None and len(high_conf_fraud) > 0:\n",
    "    actual_fraud_in_high_conf = high_conf_fraud['actual'].sum()\n",
    "    print(f\"Actually fraud: {actual_fraud_in_high_conf:,.0f} ({actual_fraud_in_high_conf/len(high_conf_fraud)*100:.1f}%)\")\n",
    "\n",
    "# %%\n",
    "# Show low-confidence predictions (uncertain)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"UNCERTAIN PREDICTIONS (0.4 <= prob <= 0.6)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "uncertain = predictions_df[(predictions_df['probability'] >= 0.4) & (predictions_df['probability'] <= 0.6)]\n",
    "print(f\"Count: {len(uncertain):,} ({len(uncertain)/len(predictions_df)*100:.2f}% of total)\")\n",
    "\n",
    "if y_test is not None and len(uncertain) > 0:\n",
    "    actual_fraud_in_uncertain = uncertain['actual'].sum()\n",
    "    print(f\"Actually fraud: {actual_fraud_in_uncertain:,.0f} ({actual_fraud_in_uncertain/len(uncertain)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INFERENCE COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
